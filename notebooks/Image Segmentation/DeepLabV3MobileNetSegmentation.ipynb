{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"eaa67cc31e1b4647b8951ebb17b09f31":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_ea85635769a14050aca54eee50623b81","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 199/199 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m30/30\u001b[0m \u001b[38;5;245m0:00:18 • 0:00:00\u001b[0m \u001b[38;5;249m1.85it/s\u001b[0m \u001b[37mv_num: 0.000 val_DICE: 0.738      \u001b[0m\n                                                                                 \u001b[37mval_IOU: 0.726 val_pixel_accuracy:\u001b[0m\n                                                                                 \u001b[37m0.904 val_pixel_f1: 0.812         \u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 199/199 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">30/30</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:18 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">1.85it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 0.000 val_DICE: 0.738      </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_IOU: 0.726 val_pixel_accuracy:</span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.904 val_pixel_f1: 0.812         </span>\n</pre>\n"},"metadata":{}}]}},"ea85635769a14050aca54eee50623b81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89ad9bd5769f4416b0f8c9065c10a046":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_5cd5071f8df04a888776e06ca83006be","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m58/58\u001b[0m \u001b[38;5;245m0:00:06 • 0:00:00\u001b[0m \u001b[38;5;249m9.57it/s\u001b[0m  \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">58/58</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:06 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">9.57it/s</span>  \n</pre>\n"},"metadata":{}}]}},"5cd5071f8df04a888776e06ca83006be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Define the complete path to your dataset\n","DATASET_PATH = '/content/drive/MyDrive/Computer Vision/50.035 CV Team 9'\n","\n","# Change directory to the dataset location\n","%cd \"/content/drive/MyDrive/Computer Vision/50.035 CV Team 9\"\n","\n","# Verify the path exists (optional check)\n","import os\n","assert os.path.exists(DATASET_PATH), \"[!] Dataset path does not exist. Please check the path.\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0s1O8Q7PVKB-","executionInfo":{"status":"ok","timestamp":1733495996343,"user_tz":-480,"elapsed":32143,"user":{"displayName":"Vainavi Kiran","userId":"01702027542169392394"}},"outputId":"3a4354fa-e05a-4e49-d8d9-ff450ad2fa11"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1Mdz9CpJD5zYhDk1e3Ch93fV4o95Ud7HJ/50.035 CV Team 9\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"pO111s4xV0lt","executionInfo":{"status":"ok","timestamp":1733496011182,"user_tz":-480,"elapsed":14846,"user":{"displayName":"Vainavi Kiran","userId":"01702027542169392394"}}},"outputs":[],"source":["%%capture\n","def is_running_in_colab():\n","    try:\n","        import google.colab\n","        return True\n","    except ImportError:\n","        return False\n","\n","if is_running_in_colab():\n","  # Normal packages\n","  %pip install lightning polars segmentation_models_pytorch\n","  # Dev packages\n","  %pip install icecream rich tqdm"]},{"cell_type":"code","source":["from pathlib import Path\n","\n","import polars as pl\n","import torch\n","import torch.nn as nn\n","from torchvision.io import decode_image\n","from torchvision.transforms import v2\n","from torchvision.tv_tensors import Image, Mask\n","import lightning as L\n","from lightning.pytorch.callbacks import RichProgressBar\n","from lightning.pytorch.loggers import CSVLogger\n","import torchmetrics\n","import torchmetrics.segmentation\n","import segmentation_models_pytorch as smp\n","\n","# Dev Imports\n","from icecream import ic\n","\n","class SegmentationData(L.LightningDataModule):\n","    def __init__(self, ws_root: Path = Path(\".\"), num_workers=0):\n","        super().__init__()\n","        self.data_path = ws_root / 'segmentation_dataset' / 'data'\n","        self.image_names = list(f.stem for f in (self.data_path / \"masks\").iterdir())\n","\n","        self.dataloader_extras = dict(\n","            num_workers = num_workers,\n","            pin_memory = True,\n","            persistent_workers = num_workers > 0\n","        )\n","\n","        self.n_classes = 1\n","\n","    def setup(self, stage: str):\n","        train, val, test = torch.utils.data.random_split(self.image_names, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(42))\n","        self.train_ds = ImageDataset(train, self.data_path, training=True)\n","        self.val_ds = ImageDataset(val, self.data_path, training=True)\n","        self.test_ds = ImageDataset(test, self.data_path)\n","\n","    def train_dataloader(self):\n","        return torch.utils.data.DataLoader(self.train_ds, batch_size=16, shuffle=True, **self.dataloader_extras)\n","\n","    def val_dataloader(self):\n","        return torch.utils.data.DataLoader(self.val_ds, batch_size=64, **self.dataloader_extras)\n","\n","    def test_dataloader(self):\n","        return torch.utils.data.DataLoader(self.test_ds, batch_size=1, **self.dataloader_extras)\n","\n","class ImageDataset(torch.utils.data.Dataset):\n","    def __init__(self, image_names, data_path, training=False):\n","        super().__init__()\n","        self.mask_path = data_path / \"masks\"\n","        self.image_path = data_path / \"images\"\n","        self.image_names = image_names\n","\n","        self.training = training\n","        self.train_transforms = v2.Compose([\n","            v2.RandomHorizontalFlip(),\n","            v2.RandomVerticalFlip(),\n","        ])\n","        self.transforms = v2.Compose([\n","            v2.RandomResizedCrop((256, 256)),\n","            v2.ToDtype(torch.float32, scale=True),\n","            v2.ToPureTensor(),\n","        ])\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        image = Image(decode_image(self.image_path / f\"{self.image_names[idx]}.jpg\", mode=\"RGB\"))\n","        mask = Mask(decode_image(self.mask_path / f\"{self.image_names[idx]}.png\", mode=\"GRAY\"))\n","        if self.training:\n","            image, mask = self.train_transforms(image, mask)\n","        image, mask = self.transforms(image, mask)\n","        mask = (mask > 37).to(torch.long).squeeze()\n","        return image, mask\n","\n","class WrappedModel(nn.Module):\n","    def __init__(self, n_classes):\n","        super().__init__()\n","        self.model = smp.DeepLabV3(\n","            classes=n_classes,\n","            encoder_name=\"mobilenet_v2\",\n","            encoder_weights=\"imagenet\"\n","        )\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        return x\n","\n","class LitWrappedModel(L.LightningModule):\n","    def __init__(self, n_classes):\n","        super().__init__()\n","        self.model = WrappedModel(n_classes)\n","        self.n_classes = n_classes\n","\n","        self.val_metrics = torchmetrics.MetricCollection(\n","            {\n","                \"pixel_accuracy\": torchmetrics.classification.Accuracy(task=\"binary\", num_classes=n_classes),\n","                \"pixel_f1\": torchmetrics.classification.F1Score(task=\"binary\", num_classes=n_classes),\n","                \"DICE\": torchmetrics.segmentation.GeneralizedDiceScore(num_classes=2, input_format=\"index\"),\n","                \"IOU\": torchmetrics.segmentation.MeanIoU(num_classes=2, input_format=\"index\"),\n","            },\n","            prefix=\"val_\",\n","        )\n","        self.test_metrics = self.val_metrics.clone(prefix=\"test_\")\n","\n","        self.losses = [smp.losses.DiceLoss('binary'), smp.losses.SoftBCEWithLogitsLoss()]\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_pred = self.model(x).squeeze(1)\n","        loss = sum(loss(y_pred, y.to(torch.float32)) for loss in self.losses)\n","        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_pred = self.model(x)\n","        y_pred = (y_pred > 0).to(torch.long).squeeze(1)\n","        self.log_dict(self.val_metrics(y_pred, y), prog_bar=True)\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_pred = self.model(x)\n","        y_pred = (y_pred > 0).to(torch.long).squeeze(1)\n","        self.log_dict(self.test_metrics(y_pred, y), prog_bar=True)\n","\n","    def on_validation_epoch_end(self):\n","        L.pytorch.utilities.memory.garbage_collection_cuda()\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=1e-3)\n","\n","# Main execution\n","exp_name = \"DeepLabV3_MobileNet\"\n","segmentation_data = SegmentationData(num_workers=15)\n","lit_model = LitWrappedModel(segmentation_data.n_classes)\n","\n","trainer = L.Trainer(\n","    max_epochs=200,\n","    accelerator='gpu',\n","    callbacks=[RichProgressBar()],\n","    logger=CSVLogger(\"csv_logs/segmentation\", name=exp_name, version=0)\n",")\n","trainer.fit(model=lit_model, datamodule=segmentation_data)\n","\n","# Save models\n","model_save_path = Path(\"models\") / \"segmentation\" / exp_name\n","model_save_path.mkdir(exist_ok=True, parents=True)\n","\n","model = lit_model.model\n","model = model.eval().cpu()\n","\n","# Save weights\n","torch.save(model.state_dict(), model_save_path / f\"weights_{exp_name}.pt\")\n","\n","# Save full model\n","torch.save(model, model_save_path / f\"model_{exp_name}.pt\")\n","\n","# Try simpler TorchScript export\n","try:\n","    # Trace the model with example input\n","    example_input = torch.randn(1, 3, 512, 512)\n","    traced_model = torch.jit.trace(model, example_input)\n","    torch.jit.save(traced_model, model_save_path / f\"traced_{exp_name}.pt\")\n","    print(\"Successfully exported traced model\")\n","except Exception as e:\n","    print(f\"Tracing failed with error: {e}\")\n","\n","# Test the model\n","trainer.test(model=lit_model, datamodule=segmentation_data)"],"metadata":{"id":"GI2Ek0gzgCcv","executionInfo":{"status":"ok","timestamp":1733501471664,"user_tz":-480,"elapsed":5460487,"user":{"displayName":"Vainavi Kiran","userId":"01702027542169392394"}},"colab":{"base_uri":"https://localhost:8080/","height":997,"referenced_widgets":["eaa67cc31e1b4647b8951ebb17b09f31","ea85635769a14050aca54eee50623b81","89ad9bd5769f4416b0f8c9065c10a046","5cd5071f8df04a888776e06ca83006be"]},"outputId":"eec6105e-1989-4dd3-f597-bb0ad1d68c88"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n","100%|██████████| 13.6M/13.6M [00:00<00:00, 186MB/s]\n","INFO: GPU available: True (cuda), used: True\n","INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n","INFO: TPU available: False, using: 0 TPU cores\n","INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n","INFO: HPU available: False, using: 0 HPUs\n","INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n","/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory csv_logs/segmentation/DeepLabV3_MobileNet/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n","/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory csv_logs/segmentation/DeepLabV3_MobileNet/version_0/checkpoints exists and is not empty.\n","INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"output_type":"display_data","data":{"text/plain":["┏━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n","┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n","┡━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n","│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model        │ WrappedModel     │ 12.6 M │ train │\n","│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ val_metrics  │ MetricCollection │      0 │ train │\n","│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ test_metrics │ MetricCollection │      0 │ train │\n","└───┴──────────────┴──────────────────┴────────┴───────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n","┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name         </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n","┡━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n","│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model        │ WrappedModel     │ 12.6 M │ train │\n","│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ val_metrics  │ MetricCollection │      0 │ train │\n","│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ test_metrics │ MetricCollection │      0 │ train │\n","└───┴──────────────┴──────────────────┴────────┴───────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1mTrainable params\u001b[0m: 12.6 M                                                                                           \n","\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n","\u001b[1mTotal params\u001b[0m: 12.6 M                                                                                               \n","\u001b[1mTotal estimated model params size (MB)\u001b[0m: 50                                                                         \n","\u001b[1mModules in train mode\u001b[0m: 259                                                                                         \n","\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 12.6 M                                                                                           \n","<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n","<span style=\"font-weight: bold\">Total params</span>: 12.6 M                                                                                               \n","<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 50                                                                         \n","<span style=\"font-weight: bold\">Modules in train mode</span>: 259                                                                                         \n","<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaa67cc31e1b4647b8951ebb17b09f31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will \n","create 15 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller \n","than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader \n","running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will \n","create 15 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller \n","than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader \n","running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches \n","(30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if\n","you want to see logs for the training epoch.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches \n","(30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if\n","you want to see logs for the training epoch.\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["INFO: `Trainer.fit` stopped: `max_epochs=200` reached.\n","INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=200` reached.\n"]},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/base/model.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if h % output_stride != 0 or w % output_stride != 0:\n","INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"output_type":"stream","name":"stdout","text":["Successfully exported traced model\n"]},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89ad9bd5769f4416b0f8c9065c10a046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m        test_DICE        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.755911648273468    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_IOU         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7534765005111694    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m   test_pixel_accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9089342355728149    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m      test_pixel_f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7363795638084412    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_DICE         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.755911648273468     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_IOU          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7534765005111694     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">    test_pixel_accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9089342355728149     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">       test_pixel_f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7363795638084412     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["[{'test_DICE': 0.755911648273468,\n","  'test_IOU': 0.7534765005111694,\n","  'test_pixel_accuracy': 0.9089342355728149,\n","  'test_pixel_f1': 0.7363795638084412}]"]},"metadata":{},"execution_count":3}]}]}